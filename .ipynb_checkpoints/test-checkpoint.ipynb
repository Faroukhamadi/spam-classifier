{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "26d24710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "21755124",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "SPAM_PATHS = [\n",
    "    '20021010_easy_ham.tar.bz2',\n",
    "    '20021010_hard_ham.tar.bz2',\n",
    "    '20021010_spam.tar.bz2',\n",
    "    '20030228_easy_ham.tar.bz2',\n",
    "    '20030228_easy_ham_2.tar.bz2',\n",
    "    '20030228_hard_ham.tar.bz2',\n",
    "    '20030228_spam.tar.bz2',\n",
    "    '20030228_spam_2.tar.bz2',\n",
    "    '20050311_spam_2.tar.bz2',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e093425",
   "metadata": {},
   "source": [
    "### Concatenating all datasets might not work!! Too much data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "0532bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_spam_data(download_root=DOWNLOAD_ROOT, spam_paths=SPAM_PATHS):\n",
    "    if not os.path.isdir('data'):\n",
    "        os.makedirs('data')\n",
    "    for path in SPAM_PATHS:\n",
    "        url = DOWNLOAD_ROOT + path\n",
    "        tgz_path = os.path.join('data', path)\n",
    "        urllib.request.urlretrieve(url, tgz_path)\n",
    "        spam_tgz = tarfile.open(tgz_path)\n",
    "        spam_tgz.extractall(path='data')\n",
    "    spam_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "de89fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67725b",
   "metadata": {},
   "source": [
    "### One spam example + one ham example should do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4f6103dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    ham_path = os.path.join(os.getcwd(), 'data', 'easy_ham_2')\n",
    "    spam_path = os.path.join(os.getcwd(), 'data', 'spam_2')\n",
    "    \n",
    "    x_spam = []\n",
    "    x_ham = []\n",
    "    \n",
    "    for filename in os.listdir(ham_path):\n",
    "        abs_path = os.path.join(ham_path, filename)\n",
    "        with open(abs_path, 'rb') as f:\n",
    "            x_ham.append(f.read())\n",
    "\n",
    "    for filename in os.listdir(spam_path):\n",
    "        abs_path = os.path.join(spam_path, filename)\n",
    "        with open(abs_path, 'rb') as f:\n",
    "            x_spam.append(f.read())\n",
    "            \n",
    "    return np.array(x_spam, dtype='object'), np.array(x_ham, dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82793e",
   "metadata": {},
   "source": [
    "## Warning! Don't use the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "598d50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    If OpenAI ever wants to test their TPUs, this is a good place to start.\n",
    "    Not feasible on a normal computer though :(\n",
    "    \"\"\"\n",
    "    x_spam = []\n",
    "    x_ham = []\n",
    "    path = os.path.join(os.getcwd(), 'data')\n",
    "    for dirname in os.listdir(path):\n",
    "        if 'tar' not in dirname:\n",
    "            if 'spam' in dirname:\n",
    "                for filename in os.listdir(os.path.join(path, dirname)):\n",
    "                    abs_path = os.path.join(path, dirname, filename)\n",
    "                    with open(abs_path, 'rb') as f:\n",
    "                        x_spam.append(f.read().split())\n",
    "            else:\n",
    "                for filename in os.listdir(os.path.join(path, dirname)):\n",
    "                    abs_path = os.path.join(path, dirname, filename)\n",
    "                    with open(abs_path, 'rb') as f:\n",
    "                        x_ham.append(f.read().split())\n",
    "    return np.array(x_spam), np.array(x_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cc01ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ham_nd = np.concatenate(x_ham).flatten()\n",
    "# x_spam_nd = np.concatenate(x_spam).flatten()\n",
    "# x = np.concatenate((x_ham_nd, x_spam_nd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2f9302e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spam, x_ham = load_data()\n",
    "y_spam = np.ones(x_spam.shape)\n",
    "y_ham = np.zeros(x_ham.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7f73da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([x_spam, x_ham])\n",
    "y = np.concatenate([y_spam, y_ham])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43bf992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "64b2b6cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [194]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m email_bow \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m email_bow\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1276\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "email_bow = vectorizer.fit_transform(x)\n",
    "email_bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "013f9869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa8cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a8378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc6931f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260595c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b07f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
